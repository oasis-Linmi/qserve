Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
Traceback (most recent call last):
  File "/data03/huhuanqi/projects/qserve/qserve_benchmark.py", line 11, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

==================================================
Benchmarking failed for model: /data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128 with batch size 4
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
Traceback (most recent call last):
  File "/data03/huhuanqi/projects/qserve/qserve_benchmark.py", line 11, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

==================================================
Benchmarking failed for model: /data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128 with batch size 4
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
Traceback (most recent call last):
  File "/data03/huhuanqi/projects/qserve/qserve_benchmark.py", line 11, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

==================================================
Benchmarking failed for model: /data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128 with batch size 4
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
Traceback (most recent call last):
  File "/data03/huhuanqi/projects/qserve/qserve_benchmark.py", line 11, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

==================================================
Benchmarking failed for model: /data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128 with batch size 4
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:23:07 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:23:11 model_runner.py:310] # GPU blocks: 100, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([4, 2, 24])
Prefill time: 0.6462838649749756
Decode time: 3.4400057792663574
Round 0 Throughput: 594.1850482692851 tokens / second.
[Warmup Round 1]
INFO 04-14 21:23:16 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:23:17 model_runner.py:310] # GPU blocks: 100, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([4, 2, 24])
Prefill time: 0.17623519897460938
Decode time: 3.433568239212036
Round 1 Throughput: 595.2990759458662 tokens / second.
INFO 04-14 21:23:21 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:23:23 model_runner.py:310] # GPU blocks: 100, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([4, 2, 24])
Prefill time: 0.17638087272644043
Decode time: 3.4291560649871826
Round 2 Throughput: 596.0650262815145 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:23:29 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:23:32 model_runner.py:310] # GPU blocks: 200, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([8, 2, 24])
Prefill time: 0.8137967586517334
Decode time: 3.540830373764038
Round 0 Throughput: 1154.5314427627607 tokens / second.
[Warmup Round 1]
INFO 04-14 21:23:37 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:23:39 model_runner.py:310] # GPU blocks: 200, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([8, 2, 24])
Prefill time: 0.3428637981414795
Decode time: 3.5180938243865967
Round 1 Throughput: 1161.9928870750825 tokens / second.
INFO 04-14 21:23:43 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:23:44 model_runner.py:310] # GPU blocks: 200, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([8, 2, 24])
Prefill time: 0.3433959484100342
Decode time: 3.7036349773406982
Round 2 Throughput: 1103.7804818808806 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:23:50 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:23:54 model_runner.py:310] # GPU blocks: 300, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([12, 2, 24])
Prefill time: 0.9716963768005371
Decode time: 3.7646586894989014
Round 0 Throughput: 1628.8329184009524 tokens / second.
[Warmup Round 1]
INFO 04-14 21:23:59 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:24:01 model_runner.py:310] # GPU blocks: 300, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([12, 2, 24])
Prefill time: 0.5110573768615723
Decode time: 3.7714967727661133
Round 1 Throughput: 1625.8796890080944 tokens / second.
INFO 04-14 21:24:06 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:24:08 model_runner.py:310] # GPU blocks: 300, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([12, 2, 24])
Prefill time: 0.5107123851776123
Decode time: 3.76816725730896
Round 2 Throughput: 1627.316300279933 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:24:15 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:24:19 model_runner.py:310] # GPU blocks: 400, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([16, 2, 24])
Prefill time: 1.318457841873169
Decode time: 4.116370439529419
Round 0 Throughput: 1986.215798628336 tokens / second.
[Warmup Round 1]
INFO 04-14 21:24:25 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:24:26 model_runner.py:310] # GPU blocks: 400, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([16, 2, 24])
Prefill time: 0.676713228225708
Decode time: 3.9959709644317627
Round 1 Throughput: 2046.0609130483629 tokens / second.
INFO 04-14 21:24:31 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:24:33 model_runner.py:310] # GPU blocks: 400, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([16, 2, 24])
Prefill time: 0.676586389541626
Decode time: 3.9954826831817627
Round 2 Throughput: 2046.3109587273007 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:24:41 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:24:45 model_runner.py:310] # GPU blocks: 500, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([20, 2, 24])
Prefill time: 1.3959190845489502
Decode time: 4.251916408538818
Round 0 Throughput: 2403.622041928178 tokens / second.
[Warmup Round 1]
INFO 04-14 21:24:51 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:24:53 model_runner.py:310] # GPU blocks: 500, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([20, 2, 24])
Prefill time: 1.3786063194274902
Decode time: 4.2464776039123535
Round 1 Throughput: 2406.7005535562316 tokens / second.
INFO 04-14 21:24:59 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:25:01 model_runner.py:310] # GPU blocks: 500, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([20, 2, 24])
Prefill time: 0.8385281562805176
Decode time: 4.278088569641113
Round 2 Throughput: 2388.9173479307724 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:25:09 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:25:13 model_runner.py:310] # GPU blocks: 600, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([24, 2, 24])
Prefill time: 1.9757816791534424
Decode time: 4.48110556602478
Round 0 Throughput: 2736.8246115387724 tokens / second.
[Warmup Round 1]
INFO 04-14 21:25:19 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:25:22 model_runner.py:310] # GPU blocks: 600, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([24, 2, 24])
Prefill time: 1.0046870708465576
Decode time: 4.472846984863281
Round 1 Throughput: 2741.877833402983 tokens / second.
INFO 04-14 21:25:27 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:25:29 model_runner.py:310] # GPU blocks: 600, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([24, 2, 24])
Prefill time: 1.00459885597229
Decode time: 4.461610794067383
Round 2 Throughput: 2748.7830216628213 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:25:37 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:25:41 model_runner.py:310] # GPU blocks: 700, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([28, 2, 24])
Prefill time: 1.8703491687774658
Decode time: 4.743999719619751
Round 0 Throughput: 3016.0204143407577 tokens / second.
[Warmup Round 1]
INFO 04-14 21:25:47 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:25:49 model_runner.py:310] # GPU blocks: 700, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([28, 2, 24])
Prefill time: 1.187802791595459
Decode time: 4.728821754455566
Round 1 Throughput: 3025.7008495866417 tokens / second.
INFO 04-14 21:25:55 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:25:57 model_runner.py:310] # GPU blocks: 700, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([28, 2, 24])
Prefill time: 1.1713223457336426
Decode time: 4.7187089920043945
Round 2 Throughput: 3032.1852914100355 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:26:06 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:26:10 model_runner.py:310] # GPU blocks: 800, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([32, 2, 24])
Prefill time: 1.8189101219177246
Decode time: 4.931288719177246
Round 0 Throughput: 3315.9688939746825 tokens / second.
[Warmup Round 1]
INFO 04-14 21:26:17 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:26:19 model_runner.py:310] # GPU blocks: 800, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([32, 2, 24])
Prefill time: 1.337386131286621
Decode time: 4.91011381149292
Round 1 Throughput: 3330.269038107729 tokens / second.
INFO 04-14 21:26:25 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:26:27 model_runner.py:310] # GPU blocks: 800, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([32, 2, 24])
Prefill time: 1.3375375270843506
Decode time: 4.901282548904419
Round 2 Throughput: 3336.269606341131 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:26:36 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:26:39 model_runner.py:310] # GPU blocks: 900, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([36, 2, 24])
Prefill time: 2.038041114807129
Decode time: 5.60224461555481
Round 0 Throughput: 3283.6838200393686 tokens / second.
[Warmup Round 1]
INFO 04-14 21:26:47 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:26:49 model_runner.py:310] # GPU blocks: 900, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([36, 2, 24])
Prefill time: 1.4980173110961914
Decode time: 5.598578214645386
Round 1 Throughput: 3285.834241250339 tokens / second.
INFO 04-14 21:26:56 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:26:58 model_runner.py:310] # GPU blocks: 900, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([36, 2, 24])
Prefill time: 1.4977059364318848
Decode time: 5.58840799331665
Round 2 Throughput: 3291.8140590308267 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:27:07 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:27:11 model_runner.py:310] # GPU blocks: 1000, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([40, 2, 24])
Prefill time: 2.116344928741455
Decode time: 5.823634147644043
Round 0 Throughput: 3509.835865680028 tokens / second.
[Warmup Round 1]
INFO 04-14 21:27:19 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:27:21 model_runner.py:310] # GPU blocks: 1000, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([40, 2, 24])
Prefill time: 1.6652319431304932
Decode time: 5.814524412155151
Round 1 Throughput: 3515.334798022444 tokens / second.
INFO 04-14 21:27:28 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:27:30 model_runner.py:310] # GPU blocks: 1000, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([40, 2, 24])
Prefill time: 1.665158987045288
Decode time: 5.784806489944458
Round 2 Throughput: 3533.3939061799542 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:27:40 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:27:44 model_runner.py:310] # GPU blocks: 1100, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([44, 2, 24])
Prefill time: 2.3105714321136475
Decode time: 6.103203535079956
Round 0 Throughput: 3683.9669315903693 tokens / second.
[Warmup Round 1]
INFO 04-14 21:27:53 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:27:55 model_runner.py:310] # GPU blocks: 1100, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([44, 2, 24])
Prefill time: 1.832266092300415
Decode time: 6.06261134147644
Round 1 Throughput: 3708.632919642912 tokens / second.
INFO 04-14 21:28:03 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:28:05 model_runner.py:310] # GPU blocks: 1100, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([44, 2, 24])
Prefill time: 1.8312220573425293
Decode time: 6.042491912841797
Round 2 Throughput: 3720.9813971311924 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:28:15 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:28:19 model_runner.py:310] # GPU blocks: 1200, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([48, 2, 24])
Prefill time: 2.928327798843384
Decode time: 6.321027755737305
Round 0 Throughput: 3880.381632201673 tokens / second.
[Warmup Round 1]
INFO 04-14 21:28:29 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:28:31 model_runner.py:310] # GPU blocks: 1200, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([48, 2, 24])
Prefill time: 1.9989993572235107
Decode time: 6.286933898925781
Round 1 Throughput: 3901.424827162726 tokens / second.
INFO 04-14 21:28:40 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:28:42 model_runner.py:310] # GPU blocks: 1200, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([48, 2, 24])
Prefill time: 2.0042226314544678
Decode time: 6.2786033153533936
Round 2 Throughput: 3906.6013200770963 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:28:52 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:28:56 model_runner.py:310] # GPU blocks: 1300, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([52, 2, 24])
Prefill time: 2.61613130569458
Decode time: 6.542909145355225
Round 0 Throughput: 4061.190429163046 tokens / second.
[Warmup Round 1]
INFO 04-14 21:29:05 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:29:07 model_runner.py:310] # GPU blocks: 1300, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([52, 2, 24])
Prefill time: 2.1611557006835938
Decode time: 6.528554916381836
Round 1 Throughput: 4070.119703416137 tokens / second.
INFO 04-14 21:29:16 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:29:18 model_runner.py:310] # GPU blocks: 1300, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([52, 2, 24])
Prefill time: 2.159489631652832
Decode time: 6.5221946239471436
Round 2 Throughput: 4074.088789444769 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:29:29 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:29:33 model_runner.py:310] # GPU blocks: 1400, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([56, 2, 24])
Prefill time: 2.7922797203063965
Decode time: 6.732231855392456
Round 0 Throughput: 4250.596327439145 tokens / second.
[Warmup Round 1]
INFO 04-14 21:29:43 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:29:45 model_runner.py:310] # GPU blocks: 1400, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([56, 2, 24])
Prefill time: 2.3274526596069336
Decode time: 6.718558311462402
Round 1 Throughput: 4259.247099363385 tokens / second.
INFO 04-14 21:29:54 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:29:56 model_runner.py:310] # GPU blocks: 1400, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([56, 2, 24])
Prefill time: 2.3268935680389404
Decode time: 6.7148730754852295
Round 2 Throughput: 4261.5846462492 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:30:07 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:30:10 model_runner.py:310] # GPU blocks: 1500, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([60, 2, 24])
Prefill time: 3.159904718399048
Decode time: 6.970175504684448
Round 0 Throughput: 4398.7414634530105 tokens / second.
[Warmup Round 1]
INFO 04-14 21:30:21 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:30:23 model_runner.py:310] # GPU blocks: 1500, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([60, 2, 24])
Prefill time: 2.4925267696380615
Decode time: 6.967217922210693
Round 1 Throughput: 4400.608728235617 tokens / second.
INFO 04-14 21:30:33 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:30:35 model_runner.py:310] # GPU blocks: 1500, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([60, 2, 24])
Prefill time: 2.4914233684539795
Decode time: 6.9630420207977295
Round 2 Throughput: 4403.2478776406115 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:30:46 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:30:51 model_runner.py:310] # GPU blocks: 1600, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([64, 2, 24])
Prefill time: 3.222503662109375
Decode time: 7.233917474746704
Round 0 Throughput: 4520.92522677626 tokens / second.
[Warmup Round 1]
INFO 04-14 21:31:02 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:31:04 model_runner.py:310] # GPU blocks: 1600, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([64, 2, 24])
Prefill time: 2.658940315246582
Decode time: 7.20868992805481
Round 1 Throughput: 4536.746666370326 tokens / second.
INFO 04-14 21:31:15 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:31:16 model_runner.py:310] # GPU blocks: 1600, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([64, 2, 24])
Prefill time: 2.658602237701416
Decode time: 7.20774245262146
Round 2 Throughput: 4537.343032852892 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:31:29 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:31:33 model_runner.py:310] # GPU blocks: 1700, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([68, 2, 24])
Prefill time: 3.4810569286346436
Decode time: 8.665826797485352
Round 0 Throughput: 4009.773194415006 tokens / second.
[Warmup Round 1]
INFO 04-14 21:31:45 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:31:47 model_runner.py:310] # GPU blocks: 1700, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([68, 2, 24])
Prefill time: 2.819279432296753
Decode time: 8.659833431243896
Round 1 Throughput: 4012.5483100670685 tokens / second.
INFO 04-14 21:31:59 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:32:01 model_runner.py:310] # GPU blocks: 1700, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([68, 2, 24])
Prefill time: 2.8192200660705566
Decode time: 8.659642934799194
Round 2 Throughput: 4012.636578855172 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:32:14 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:32:18 model_runner.py:310] # GPU blocks: 1800, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([72, 2, 24])
Prefill time: 3.8780360221862793
Decode time: 8.908684968948364
Round 0 Throughput: 4129.902463521858 tokens / second.
[Warmup Round 1]
INFO 04-14 21:32:31 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:32:33 model_runner.py:310] # GPU blocks: 1800, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([72, 2, 24])
Prefill time: 2.9859325885772705
Decode time: 8.885839939117432
Round 1 Throughput: 4140.520226797411 tokens / second.
INFO 04-14 21:32:45 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:32:47 model_runner.py:310] # GPU blocks: 1800, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([72, 2, 24])
Prefill time: 2.98564076423645
Decode time: 8.878811836242676
Round 2 Throughput: 4143.797692594147 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:33:01 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:33:05 model_runner.py:310] # GPU blocks: 1900, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([76, 2, 24])
Prefill time: 3.9765117168426514
Decode time: 9.108550786972046
Round 0 Throughput: 4263.685948323097 tokens / second.
[Warmup Round 1]
INFO 04-14 21:33:18 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:33:20 model_runner.py:310] # GPU blocks: 1900, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([76, 2, 24])
Prefill time: 3.1534483432769775
Decode time: 9.093599796295166
Round 1 Throughput: 4270.695969688727 tokens / second.
INFO 04-14 21:33:32 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:33:34 model_runner.py:310] # GPU blocks: 1900, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([76, 2, 24])
Prefill time: 3.152808427810669
Decode time: 9.086949586868286
Round 2 Throughput: 4273.821443460257 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:33:49 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:33:54 model_runner.py:310] # GPU blocks: 2000, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([80, 2, 24])
Prefill time: 3.784395456314087
Decode time: 9.37655258178711
Round 0 Throughput: 4359.811310545495 tokens / second.
[Warmup Round 1]
INFO 04-14 21:34:07 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:34:09 model_runner.py:310] # GPU blocks: 2000, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([80, 2, 24])
Prefill time: 3.3217368125915527
Decode time: 9.344591617584229
Round 1 Throughput: 4374.723013371057 tokens / second.
INFO 04-14 21:34:22 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:34:23 model_runner.py:310] # GPU blocks: 2000, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([80, 2, 24])
Prefill time: 3.3211307525634766
Decode time: 9.343932867050171
Round 2 Throughput: 4375.03143287304 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:34:39 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:34:43 model_runner.py:310] # GPU blocks: 2100, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([84, 2, 24])
Prefill time: 4.128403663635254
Decode time: 9.597442626953125
Round 0 Throughput: 4472.441427204131 tokens / second.
[Warmup Round 1]
INFO 04-14 21:34:58 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:35:00 model_runner.py:310] # GPU blocks: 2100, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([84, 2, 24])
Prefill time: 3.481069564819336
Decode time: 9.577513456344604
Round 1 Throughput: 4481.747814362515 tokens / second.
INFO 04-14 21:35:13 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:35:15 model_runner.py:310] # GPU blocks: 2100, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([84, 2, 24])
Prefill time: 3.4822938442230225
Decode time: 9.577576875686646
Round 2 Throughput: 4481.7181378064015 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:35:31 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:35:34 model_runner.py:310] # GPU blocks: 2200, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([88, 2, 24])
Prefill time: 4.370981454849243
Decode time: 9.793210983276367
Round 0 Throughput: 4591.752396307072 tokens / second.
[Warmup Round 1]
INFO 04-14 21:35:49 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:35:50 model_runner.py:310] # GPU blocks: 2200, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([88, 2, 24])
Prefill time: 3.66640305519104
Decode time: 9.785642862319946
Round 1 Throughput: 4595.303612923713 tokens / second.
INFO 04-14 21:36:04 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:36:06 model_runner.py:310] # GPU blocks: 2200, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([88, 2, 24])
Prefill time: 3.659761667251587
Decode time: 9.776027202606201
Round 2 Throughput: 4599.823534452925 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:36:21 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:36:26 model_runner.py:310] # GPU blocks: 2300, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([92, 2, 24])
Prefill time: 4.280256509780884
Decode time: 10.020082950592041
Round 0 Throughput: 4691.777526374896 tokens / second.
[Warmup Round 1]
INFO 04-14 21:36:41 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:36:43 model_runner.py:310] # GPU blocks: 2300, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([92, 2, 24])
Prefill time: 3.8124446868896484
Decode time: 10.005299091339111
Round 1 Throughput: 4698.710110594795 tokens / second.
INFO 04-14 21:36:56 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:36:58 model_runner.py:310] # GPU blocks: 2300, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([92, 2, 24])
Prefill time: 3.8124139308929443
Decode time: 9.995564460754395
Round 2 Throughput: 4703.286161035058 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:37:14 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:37:18 model_runner.py:310] # GPU blocks: 2400, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([96, 2, 24])
Prefill time: 4.439586639404297
Decode time: 10.269292831420898
Round 0 Throughput: 4776.959894444107 tokens / second.
[Warmup Round 1]
INFO 04-14 21:37:33 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:37:35 model_runner.py:310] # GPU blocks: 2400, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([96, 2, 24])
Prefill time: 3.978663206100464
Decode time: 10.264444351196289
Round 1 Throughput: 4779.21632399738 tokens / second.
INFO 04-14 21:37:50 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:37:52 model_runner.py:310] # GPU blocks: 2400, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([96, 2, 24])
Prefill time: 3.9771671295166016
Decode time: 10.261719942092896
Round 2 Throughput: 4780.485169817931 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:38:08 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:38:12 model_runner.py:310] # GPU blocks: 2500, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([100, 2, 24])
Prefill time: 4.606295347213745
Decode time: 10.49452829360962
Round 0 Throughput: 4869.204081436997 tokens / second.
[Warmup Round 1]
INFO 04-14 21:38:27 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:38:29 model_runner.py:310] # GPU blocks: 2500, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([100, 2, 24])
Prefill time: 4.139292240142822
Decode time: 10.467777252197266
Round 1 Throughput: 4881.647628609381 tokens / second.
INFO 04-14 21:38:44 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:38:46 model_runner.py:310] # GPU blocks: 2500, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([100, 2, 24])
Prefill time: 4.13927149772644
Decode time: 10.459803581237793
Round 2 Throughput: 4885.368984524748 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:39:03 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:39:07 model_runner.py:310] # GPU blocks: 2600, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([104, 2, 24])
Prefill time: 4.768472671508789
Decode time: 10.726056814193726
Round 0 Throughput: 4954.663295245171 tokens / second.
[Warmup Round 1]
INFO 04-14 21:39:22 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:39:24 model_runner.py:310] # GPU blocks: 2600, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([104, 2, 24])
Prefill time: 4.306476593017578
Decode time: 10.710465431213379
Round 1 Throughput: 4961.875871904043 tokens / second.
INFO 04-14 21:39:39 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:39:41 model_runner.py:310] # GPU blocks: 2600, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([104, 2, 24])
Prefill time: 4.306241512298584
Decode time: 10.703963994979858
Round 2 Throughput: 4964.8896450814345 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:39:59 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:40:03 model_runner.py:310] # GPU blocks: 2700, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([108, 2, 24])
Prefill time: 4.94618034362793
Decode time: 10.9142005443573
Round 0 Throughput: 5056.531605380157 tokens / second.
[Warmup Round 1]
INFO 04-14 21:40:19 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:40:21 model_runner.py:310] # GPU blocks: 2700, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([108, 2, 24])
Prefill time: 4.496956825256348
Decode time: 10.904129028320312
Round 1 Throughput: 5061.202032428741 tokens / second.
INFO 04-14 21:40:37 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:40:38 model_runner.py:310] # GPU blocks: 2700, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([108, 2, 24])
Prefill time: 4.476354598999023
Decode time: 10.899561405181885
Round 2 Throughput: 5063.323004333224 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:40:56 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:41:00 model_runner.py:310] # GPU blocks: 2800, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([112, 2, 24])
Prefill time: 5.440295457839966
Decode time: 11.189321041107178
Round 0 Throughput: 5114.8769250378855 tokens / second.
[Warmup Round 1]
INFO 04-14 21:41:18 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:41:20 model_runner.py:310] # GPU blocks: 2800, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([112, 2, 24])
Prefill time: 4.64043927192688
Decode time: 11.164630651473999
Round 1 Throughput: 5126.188387830277 tokens / second.
INFO 04-14 21:41:36 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:41:37 model_runner.py:310] # GPU blocks: 2800, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([112, 2, 24])
Prefill time: 4.640429496765137
Decode time: 11.137220621109009
Round 2 Throughput: 5138.804549810652 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:41:55 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:41:59 model_runner.py:310] # GPU blocks: 2900, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([116, 2, 24])
Prefill time: 5.706088304519653
Decode time: 11.373541116714478
Round 0 Throughput: 5211.745347531949 tokens / second.
[Warmup Round 1]
INFO 04-14 21:42:16 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:42:18 model_runner.py:310] # GPU blocks: 2900, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([116, 2, 24])
Prefill time: 4.804909706115723
Decode time: 11.353120565414429
Round 1 Throughput: 5221.119573113264 tokens / second.
INFO 04-14 21:42:35 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:42:36 model_runner.py:310] # GPU blocks: 2900, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([116, 2, 24])
Prefill time: 4.804068326950073
Decode time: 11.354424238204956
Round 2 Throughput: 5220.520103568991 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:42:55 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:42:59 model_runner.py:310] # GPU blocks: 3000, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([120, 2, 24])
Prefill time: 5.753647327423096
Decode time: 11.629398584365845
Round 0 Throughput: 5272.843608820533 tokens / second.
[Warmup Round 1]
INFO 04-14 21:43:16 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:43:18 model_runner.py:310] # GPU blocks: 3000, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([120, 2, 24])
Prefill time: 4.976816654205322
Decode time: 11.628662109375
Round 1 Throughput: 5273.177552434338 tokens / second.
INFO 04-14 21:43:35 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:43:37 model_runner.py:310] # GPU blocks: 3000, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([120, 2, 24])
Prefill time: 4.966701507568359
Decode time: 11.624335765838623
Round 2 Throughput: 5275.140122862422 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:43:55 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:44:00 model_runner.py:310] # GPU blocks: 3100, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([124, 2, 24])
Prefill time: 5.713593482971191
Decode time: 11.851670026779175
Round 0 Throughput: 5346.419522044344 tokens / second.
[Warmup Round 1]
INFO 04-14 21:44:17 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:44:19 model_runner.py:310] # GPU blocks: 3100, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([124, 2, 24])
Prefill time: 5.132222652435303
Decode time: 11.819862604141235
Round 1 Throughput: 5360.806814945517 tokens / second.
INFO 04-14 21:44:36 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:44:38 model_runner.py:310] # GPU blocks: 3100, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([124, 2, 24])
Prefill time: 5.132868528366089
Decode time: 11.81848692893982
Round 2 Throughput: 5361.430814366022 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:44:57 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:45:01 model_runner.py:310] # GPU blocks: 3200, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([128, 2, 24])
Prefill time: 5.754758596420288
Decode time: 12.255251169204712
Round 0 Throughput: 5337.140716002524 tokens / second.
[Warmup Round 1]
INFO 04-14 21:45:19 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:45:21 model_runner.py:310] # GPU blocks: 3200, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([128, 2, 24])
Prefill time: 5.307168960571289
Decode time: 12.241622924804688
Round 1 Throughput: 5343.082400248297 tokens / second.
INFO 04-14 21:45:39 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:45:41 model_runner.py:310] # GPU blocks: 3200, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([128, 2, 24])
Prefill time: 5.300365447998047
Decode time: 12.234797239303589
Round 2 Throughput: 5346.063258807471 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:46:01 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:46:04 model_runner.py:310] # GPU blocks: 3300, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([132, 2, 24])
Prefill time: 5.969008922576904
Decode time: 14.78274941444397
Round 0 Throughput: 4562.885976684 tokens / second.
[Warmup Round 1]
INFO 04-14 21:46:25 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:46:27 model_runner.py:310] # GPU blocks: 3300, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([132, 2, 24])
Prefill time: 5.461616516113281
Decode time: 14.748457193374634
Round 1 Throughput: 4573.495323314298 tokens / second.
INFO 04-14 21:46:48 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:46:49 model_runner.py:310] # GPU blocks: 3300, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([132, 2, 24])
Prefill time: 5.4613964557647705
Decode time: 14.747167348861694
Round 2 Throughput: 4573.895338972097 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:47:12 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:47:16 model_runner.py:310] # GPU blocks: 3400, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([136, 2, 24])
Prefill time: 6.088378667831421
Decode time: 15.080610036849976
Round 0 Throughput: 4608.301642319787 tokens / second.
[Warmup Round 1]
INFO 04-14 21:47:38 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:47:39 model_runner.py:310] # GPU blocks: 3400, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([136, 2, 24])
Prefill time: 5.627636432647705
Decode time: 15.07077932357788
Round 1 Throughput: 4611.307650910602 tokens / second.
INFO 04-14 21:48:00 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:48:02 model_runner.py:310] # GPU blocks: 3400, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([136, 2, 24])
Prefill time: 5.630629539489746
Decode time: 15.083056211471558
Round 2 Throughput: 4607.554266564635 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:48:25 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:48:29 model_runner.py:310] # GPU blocks: 3500, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([140, 2, 24])
Prefill time: 6.980608940124512
Decode time: 15.284409284591675
Round 0 Throughput: 4680.586515837415 tokens / second.
[Warmup Round 1]
INFO 04-14 21:48:52 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:48:54 model_runner.py:310] # GPU blocks: 3500, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([140, 2, 24])
Prefill time: 6.312736988067627
Decode time: 15.2811758518219
Round 1 Throughput: 4681.576908328729 tokens / second.
INFO 04-14 21:49:16 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:49:18 model_runner.py:310] # GPU blocks: 3500, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([140, 2, 24])
Prefill time: 5.798976421356201
Decode time: 15.266678094863892
Round 2 Throughput: 4686.02269304859 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:49:41 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:49:46 model_runner.py:310] # GPU blocks: 3600, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([144, 2, 24])
Prefill time: 6.4447009563446045
Decode time: 15.546010971069336
Round 0 Throughput: 4733.304262870884 tokens / second.
[Warmup Round 1]
INFO 04-14 21:50:08 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:50:10 model_runner.py:310] # GPU blocks: 3600, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([144, 2, 24])
Prefill time: 6.023294687271118
Decode time: 15.530936241149902
Round 1 Throughput: 4737.898530871304 tokens / second.
INFO 04-14 21:50:32 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:50:34 model_runner.py:310] # GPU blocks: 3600, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([144, 2, 24])
Prefill time: 5.960999488830566
Decode time: 15.514463901519775
Round 2 Throughput: 4742.928951144217 tokens / second.
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

==================================================
Running command: python /data03/huhuanqi/projects/qserve/qserve_benchmark.py --model "/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128" --benchmarking --precision w4a8kv4 --group-size 128 
[Warmup Round 0]
INFO 04-14 21:50:57 llm_engine.py:122] Initializing an LLM engine with config: model='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer='/data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=int8, device_config=cuda, ifb_config=False, seed=0)
[INFO] Using w4a8kv4 precision
INFO 04-14 21:51:01 model_runner.py:310] # GPU blocks: 3700, # CPU blocks: 10
[INFO] USE INT4 w/ ZERO_POINTS for KV CACHE
Running without ifb mode
Running with benchmarking mode
torch.Size([148, 2, 24])
/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/data03/huhuanqi/projects/qserve/qserve_benchmark.py", line 140, in <module>
    main(args)
  File "/data03/huhuanqi/projects/qserve/qserve_benchmark.py", line 110, in main
    time_lis, num_tokens = process_requests(
  File "/data03/huhuanqi/projects/qserve/qserve_benchmark.py", line 60, in process_requests
    requests_outputs = engine.step()
  File "/data03/huhuanqi/projects/qserve/qserve/engine/llm_engine.py", line 621, in step
    all_outputs = self._run_workers(
  File "/data03/huhuanqi/projects/qserve/qserve/engine/llm_engine.py", line 737, in _run_workers
    output = executor(*args, **kwargs)
  File "/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/data03/huhuanqi/projects/qserve/qserve/worker/worker.py", line 247, in execute_model
    output = self.model_runner.execute_model(
  File "/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/data03/huhuanqi/projects/qserve/qserve/worker/model_runner.py", line 659, in execute_model
    output = model(input_tokens, input_metadata)
  File "/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data03/huhuanqi/anaconda/envs/QServe/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data03/huhuanqi/projects/qserve/qserve/modeling/models/llama_w4a8_unpad.py", line 473, in forward
    hidden_states[input_metadata.cu_seqlens[1:] - 1, :]
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


==================================================
Benchmarking failed for model: /data04/huhuanqi/models/Mistral-7B-v0.1-QServe-g128 with batch size 148
